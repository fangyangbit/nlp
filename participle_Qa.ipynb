{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: 搭建一个分词工具\n",
    "\n",
    "### Part 1.1  基于枚举方法来搭建中文分词工具\n",
    "\n",
    "此项目需要的数据：\n",
    "1. 综合类中文词库.xlsx： 包含了中文词，当做词典来用\n",
    "2. 以变量的方式提供了部分unigram概率 word_prob\n",
    "\n",
    "\n",
    "举个例子： 给定词典=[我们 学习 人工 智能 人工智能 未来 是]， 另外我们给定unigram概率：p(我们)=0.25, p(学习)=0.15, p(人工)=0.05, p(智能)=0.1, p(人工智能)=0.2, p(未来)=0.1, p(是)=0.15\n",
    "\n",
    "#### Step 1: 对于给定字符串：”我们学习人工智能，人工智能是未来“, 找出所有可能的分割方式\n",
    "- [我们，学习，人工智能，人工智能，是，未来]\n",
    "- [我们，学习，人工，智能，人工智能，是，未来]\n",
    "- [我们，学习，人工，智能，人工，智能，是，未来]\n",
    "- [我们，学习，人工智能，人工，智能，是，未来]\n",
    ".......\n",
    "\n",
    "\n",
    "#### Step 2: 我们也可以计算出每一个切分之后句子的概率\n",
    "- p(我们，学习，人工智能，人工智能，是，未来)= -log p(我们)-log p(学习)-log p(人工智能)-log p(人工智能)-log p(是)-log p(未来)\n",
    "- p(我们，学习，人工，智能，人工智能，是，未来)=-log p(我们)-log p(学习)-log p(人工)-log p(智能)-log p(人工智能)-log p(是)-log p(未来)\n",
    "- p(我们，学习，人工，智能，人工，智能，是，未来)=-log p(我们)-log p(学习)-log p(人工)-log p(智能)-log p(人工)-log p(智能)-log p(是)-log p(未来)\n",
    "- p(我们，学习，人工智能，人工，智能，是，未来)=-log p(我们)-log p(学习)-log p(人工智能)-log p(人工)-log p(智能)-log(是)-log p(未来)\n",
    ".....\n",
    "\n",
    "#### Step 3: 返回第二步中概率最大的结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.基本准备过程\n",
    "- 使用pandas读取csv,作为词典\n",
    "- 没有出现的词的概率设为default\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "path='doc.csv'\n",
    "data=pd.read_csv(path,usecols=[0],encoding='gb18030')\n",
    "data=data.to_numpy()\n",
    "data=data.tolist()\n",
    "dic_words ={}   # 保存词典库中读取的单词\n",
    "for d in data:\n",
    "    if d[0] not in dic_words:\n",
    "        dic_words[d[0]]=1\n",
    "    else:\n",
    "        dic_words[d[0]]+=1\n",
    "\n",
    "#字典存储概率\n",
    "word_prob = {\"北京\":0.03,\"的\":0.08,\"天\":0.005,\"气\":0.005,\"天气\":0.06,\"真\":0.04,\"好\":0.05,\"真好\":0.04,\"啊\":0.01,\"真好啊\":0.02, \n",
    "             \"今\":0.01,\"今天\":0.07,\"课程\":0.06,\"内容\":0.06,\"有\":0.05,\"很\":0.03,\"很有\":0.04,\"意思\":0.06,\"有意思\":0.005,\"课\":0.01,\n",
    "             \"程\":0.005,\"经常\":0.08,\"意见\":0.08,\"意\":0.01,\"见\":0.005,\"有意见\":0.02,\"分歧\":0.04,\"分\":0.02, \"歧\":0.005}\n",
    "default_prob=0.0001\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.基于匹配的分词算法\n",
    "- 最大向前匹配\n",
    "    ：设置一个最大匹配的长度，根据这个长度从头开始匹配，如果没有这样的词，那么将长度减1，再次进行匹配，直到找到相应的词或者长度为1.再重复这个过程\n",
    "- 最大向后匹配：\n",
    "  同理 只是从末尾开始\n",
    "- 双向最大匹配\n",
    "    从最大向前匹配和最大向后匹配选择比较好的结果\n",
    "- 参考博客：https://blog.csdn.net/selinda001/article/details/79345072"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def Max_forwardMatch(input_str):\n",
    "    segments = []   \n",
    "    sentence=input_str\n",
    "    Maxlen=5\n",
    "    i=0\n",
    "    forword_sub_seg=[]\n",
    "    while(i<len(sentence)):\n",
    "        max_len=Maxlen\n",
    "        while(max_len>=1):\n",
    "            if sentence[i:i+max_len] in dic_words:\n",
    "                break\n",
    "            else:\n",
    "                max_len=max_len-1\n",
    "        forword_sub_seg.append(sentence[i:i+max_len])\n",
    "        i=i+max_len\n",
    "    segments.append(forword_sub_seg)\n",
    "    return segments\n",
    "def Max_backwardMatch(input_str):\n",
    "    \n",
    "    segments=[]\n",
    "    \n",
    "    sentence=input_str\n",
    "    \n",
    "    i=len(sentence)\n",
    "   \n",
    "    Maxlen=5\n",
    "    i=len(sentence)\n",
    "    back_sub_seg=[]\n",
    "    \n",
    "    max_len=Maxlen\n",
    "    while(i>max_len):\n",
    "        while(max_len>=1):\n",
    "            if sentence[i-max_len:i] in dic_words:\n",
    "                break\n",
    "            else:\n",
    "                max_len=max_len-1\n",
    "        back_sub_seg.append(sentence[i-max_len:i])\n",
    "        i=i-max_len\n",
    "          #print(sentence[i])\n",
    "    if i>0:\n",
    "          back_sub_seg.append(sentence[:i])\n",
    "    back_sub_seg.reverse()\n",
    "    segments.append(back_sub_seg)\n",
    "    return segments\n",
    "    \n",
    "def word_segment_naive(input_str):\n",
    "    \"\"\"\n",
    "    1. 对于输入字符串做分词，并返回所有可行的分词之后的结果。\n",
    "    2. 针对于每一个返回结果，计算句子的概率\n",
    "    3. 返回概率最高的最作为最后结果\n",
    "    \"\"\"\n",
    "    segments = []  \n",
    "    \n",
    "    sentence=input_str\n",
    "  \n",
    "    forward_segments=Max_forwardMatch(input_str)\n",
    "    backward_segments=Max_backwardMatch(input_str)\n",
    "    \n",
    "    segments+=forward_segments\n",
    "    segments+=backward_segments\n",
    "  \n",
    "\n",
    "\n",
    "    # TODO: 第二步：循环所有的分词结果，并计算出概率最高的分词结果，并返回\n",
    "    best_segment = segments[0]\n",
    "    best_score = 1.0\n",
    "    for seg in segments:\n",
    "        score=0.0\n",
    "        for word in seg:\n",
    "            if word in word_prob:\n",
    "                score=score+(-math.log(word_prob[word]))\n",
    "            else:\n",
    "                score=score+(-math.log(default_prob))\n",
    "        if(score<best_score):\n",
    "            best_score=score\n",
    "            best_segment=seg\n",
    "    print(\"best:{}\".format(best_segment))\n",
    "    return best_segment      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best:['北京', '的', '天气', '真好', '啊']\n",
      "['北京', '的', '天气', '真好', '啊']\n",
      "best:['今天', '的', '课程', '内容', '很', '有意思']\n",
      "['今天', '的', '课程', '内容', '很', '有意思']\n",
      "best:['经常', '有意', '见', '分歧']\n",
      "['经常', '有意', '见', '分歧']\n"
     ]
    }
   ],
   "source": [
    "# 测试\n",
    "print (word_segment_naive(\"北京的天气真好啊\"))\n",
    "print (word_segment_naive(\"今天的课程内容很有意思\"))\n",
    "print (word_segment_naive(\"经常有意见分歧\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.2  基于维特比算法来优化上述流程\n",
    "\n",
    "此项目需要的数据：\n",
    "1. 综合类中文词库.xlsx： 包含了中文词，当做词典来用\n",
    "2. 以变量的方式提供了部分unigram概率word_prob\n",
    "\n",
    "\n",
    "举个例子： 给定词典=[我们 学习 人工 智能 人工智能 未来 是]， 另外我们给定unigram概率：p(我们)=0.25, p(学习)=0.15, p(人工)=0.05, p(智能)=0.1, p(人工智能)=0.2, p(未来)=0.1, p(是)=0.15\n",
    "\n",
    "#### Step 1: 根据词典，输入的句子和 word_prob来创建带权重的有向图（Directed Graph）\n",
    "有向图的每一条边是一个单词的概率（只要存在于词典里的都可以作为一个合法的单词），这些概率已经给出（存放在word_prob）。\n",
    "使用字典来表示邻接表:两个字典，一个存储邻接节点的下标，一个存储权重。\n",
    "\n",
    "#### Step 2: 编写维特比算法（viterebi）算法来找出其中最好的PATH， 也就是最好的句子切分\n",
    "使用dp求解最优的切分。dp[i]表示以第i个词结尾的单词的句子当前最小的权重。dp_path[i]记录最优的到达节点i的节点的下标。\n",
    "\n",
    "\n",
    "#### Step 3: 返回结果\n",
    "通过记录的路径，倒着切割句子，最后将其反转即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def word_segment_viterbi(input_str):\n",
    "    \"\"\"\n",
    "    1. 基于输入字符串，词典，以及给定的unigram概率来创建DAG(有向图）。\n",
    "    2. 编写维特比算法来寻找最优的PATH\n",
    "    3. 返回分词结果\n",
    "    \n",
    "    input_str: 输入字符串   输入格式：“今天天气好”\n",
    "    best_segment: 最好的分词结果  输出格式：[\"今天\"，\"天气\"，\"好\"]\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    #基本信息的准备\n",
    "    sentence=[]\n",
    "    for s in input_str:\n",
    "        sentence.append(s)\n",
    "    s_len=len(sentence)\n",
    "    \n",
    "    word2index={}\n",
    "    for word,index in enumerate(sentence):#编号从1开始\n",
    "        word2index[word]=index\n",
    "        \n",
    "    #使用两个dict表示邻接表\n",
    "    #一个表示与那些词有关系\n",
    "    #另一个存储那些词的权重\n",
    "    dic_link={}\n",
    "    dic_weight={}\n",
    "  \n",
    "    for i in range(s_len):\n",
    "        link=[]\n",
    "        weight=[]\n",
    "        for j in range(i+1):\n",
    "          #每个词都查看其前面的词有没有链接\n",
    "            if input_str[j:i+1] in dic_words:\n",
    "                link.append(j)\n",
    "                prob=0.0\n",
    "                if input_str[j:i+1] in word_prob:\n",
    "                    prob=-math.log(word_prob[input_str[j:i+1]])\n",
    "                else:\n",
    "                    prob=-math.log(default_prob)\n",
    "                weight.append(prob)\n",
    "        dic_link[i]=link\n",
    "        dic_weight[i]=weight\n",
    "\n",
    "              \n",
    "    #进行计算 最短路径 使用dp计算最短路径\n",
    "    dp=[0.0]*(s_len+1)\n",
    "    dp_path=[0]*(s_len+1)\n",
    "    \n",
    "    #检查一列的值就是可能右边的值\n",
    "    for i in range(1,s_len+1):\n",
    "        best_value=10000.0\n",
    "        bast_path=i-1\n",
    "        for j in range(len(dic_link[i-1])):#每个节点的临域\n",
    "                index=dic_link[i-1][j]\n",
    "                value=dp[index]+dic_weight[i-1][j]\n",
    "                if value-best_value<1e-9:\n",
    "                    best_value=value\n",
    "                    best_path=index\n",
    "        dp[i]=best_value #记录每一个节点对应点最好路径和值\n",
    "        dp_path[i]=best_path           \n",
    "                    \n",
    "    #根据最好的PATH, 返回最好的切分\n",
    "    #倒着来切割\n",
    "    path_index=s_len\n",
    "    path_list=[]\n",
    "    while path_index>0:\n",
    "        incoming_index=dp_path[path_index]\n",
    "        path_list.append(input_str[incoming_index:path_index])\n",
    "        path_index=incoming_index\n",
    "    path_list.reverse()\n",
    "    best_segment=path_list\n",
    "    \n",
    "    return best_segment      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['北京', '的', '天气', '真好', '啊']\n",
      "['今天', '的', '课程', '内容', '很', '有意思']\n",
      "['经常', '有', '意见', '分歧']\n"
     ]
    }
   ],
   "source": [
    "# 测试\n",
    "print (word_segment_viterbi(\"北京的天气真好啊\"))\n",
    "print (word_segment_viterbi(\"今天的课程内容很有意思\"))\n",
    "print (word_segment_viterbi(\"经常有意见分歧\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 2:  搭建一个简单的问答系统"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本次项目的目标是搭建一个基于检索式的简单的问答系统。\n",
    "\n",
    "通过此项目，你将会有机会掌握以下几个知识点：\n",
    "1. 字符串操作   2. 文本预处理技术（词过滤，标准化）   3. 文本的表示（tf-idf, word2vec)  4. 文本相似度计算  5. 文本高效检索\n",
    "\n",
    "此项目需要的数据：\n",
    "1. train-v2.0.json: 这个数据包含了问题和答案的pair， 但是以JSON格式存在，需要编写parser来提取出里面的问题和答案。 \n",
    "2. glove.6B: 这个文件需要从网上下载，下载地址为：https://nlp.stanford.edu/projects/glove/， 请使用d=100的词向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.1  第一部分： 读取文件，并把内容分别写到两个list里（一个list对应问题集，另一个list对应答案集）\n",
    "首先需要分析数据集的形式，然后读出question,answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "def read_corpus():\n",
    "    path='train-v2.0.json'\n",
    "    f=open(path,encoding='utf-8')\n",
    "    dic=json.load(f)\n",
    "    data=dic[\"data\"][0][\"paragraphs\"]\n",
    "    qlist=[]\n",
    "    alist=[]\n",
    "    for i in range(len(data)):\n",
    "        data_qas=data[i][\"qas\"]\n",
    "        for j in range(len(data_qas)):\n",
    "            qlist.append(data_qas[j][\"question\"])\n",
    "            answer=data_qas[j][\"answers\"][0][\"text\"]\n",
    "            alist.append(answer)\n",
    "    assert len(qlist) == len(alist)  # 确保长度一样\n",
    "    #print(qlist)\n",
    "    return qlist, alist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.2 理解数据（可视化分析/统计信息）\n",
    "对数据的理解是任何AI工作的第一步，需要充分对手上的数据有个更直观的理解。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def word2dic(wordlist):\n",
    "    #只考虑qlist\n",
    "    word2freq={}\n",
    "    for sentence in wordlist:\n",
    "        #print(sentence)\n",
    "        sentence=sentence.split()\n",
    "        for word in sentence:\n",
    "            if word in word2freq:\n",
    "                word2freq[word]+=1\n",
    "            else:\n",
    "                word2freq[word]=1    \n",
    "    #将词典按照频次排序\n",
    "    sorted(word2freq.values(),reverse=True)\n",
    "    word2index={}\n",
    "    index2word={}\n",
    "    words_total=0\n",
    "    index=0\n",
    "    for word in word2freq:\n",
    "        word2index[word]=index\n",
    "        index2word[index]=word\n",
    "        words_total+=word2freq[word]\n",
    "        index+=1\n",
    "        \n",
    "    return word2freq,word2index,index2word,words_total  \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经过可视化分析，我们可以看到词表的频率分布，满足*齐夫定律*：词的频率与其频率的排名呈反比。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def Visualization(wordlist):\n",
    "    word2freq,word2index,_,_=word2dic(wordlist)\n",
    "    word_index=[]\n",
    "    word_freq=[]\n",
    "    for word in word2freq:\n",
    "        word_index.append(word2index[word])\n",
    "        word_freq.append(word2freq[word])\n",
    "        \n",
    "    #转换成np\n",
    "    np.array(word_index)\n",
    "    np.array(word_freq)\n",
    "    \n",
    "    plt.title(\"WOrd Frequency\")\n",
    "    plt.plot(word_index,word_freq)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 文本预处理\n",
    "- 使用正则表达式去掉特殊符号\n",
    "- 使用NLTK的停用词表取出停用词\n",
    "- 使用split进行英文分词，使用lower统一大小写\n",
    "- 使用NLTK词干工具 进行词干抽取\n",
    "- 过滤低频词 设定阈值 比如5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re \n",
    "def Preprocessing(wordlist):\n",
    "    \n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "    stopword_list=nltk.corpus.stopwords.words('english')\n",
    "    \n",
    "    #step1:去除字符串的特殊符号 \n",
    "    special_character_removal = re.compile(r'[^a-z\\d ]', re.IGNORECASE)\n",
    "    for i in range(len(wordlist)):\n",
    "        wordlist[i]=special_character_removal.sub('',wordlist[i])\n",
    "    \n",
    "    #step2.取出停用词\n",
    "    for i in range(len(wordlist)):\n",
    "        tmp=[]\n",
    "        wordlist[i]=wordlist[i].split()\n",
    "        for w in wordlist[i]:\n",
    "            w=w.lower()\n",
    "            if w not in stopword_list:\n",
    "                tmp.append(w)\n",
    "        wordlist[i]=tmp\n",
    "        \n",
    "    \n",
    "    # step3:统计词频 过滤低频词\n",
    "    worddic={}\n",
    "    for q in wordlist:\n",
    "        for w in q:\n",
    "            if w not in worddic:\n",
    "                worddic[w]=1\n",
    "            else:\n",
    "                worddic[w]+=1\n",
    "                \n",
    "    for i in range(len(wordlist)):\n",
    "        tmp_q=[]\n",
    "        for w in wordlist[i]:\n",
    "            if worddic[w]>=1:\n",
    "                tmp_q.append(w)\n",
    "        wordlist[i]=tmp_q\n",
    "    \n",
    "    wordnet_Lemmatizer=WordNetLemmatizer()\n",
    "    for i in range(len(wordlist)):\n",
    "        for j in range(len(wordlist[i])):\n",
    "            wordlist[i][j]=wordnet_Lemmatizer.lemmatize(wordlist[i][j])\n",
    "            \n",
    "    return wordlist\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 文本表示\n",
    "- 将向量转化为tf_idf向量\n",
    "- 向量大小为词表长度\n",
    "- 分别求每一个词的tf和idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "def Vectorization(wordlist,x):\n",
    "    word2freq,word2index,_,word_total=word2dic(wordlist)\n",
    "    wordlist=Preprocessing(wordlist)\n",
    "    x=Preprocessing(x)\n",
    "    word2tdf={}\n",
    "    for word in word2freq:\n",
    "        tf=word2freq[word]*1.0/word_total\n",
    "        idf=0\n",
    "        for sentence in wordlist:\n",
    "            if word in sentence:\n",
    "                idf+=1\n",
    "        idf=math.log((1.0+1.0)/(idf+1.0))\n",
    "        tf_idf=tf*idf\n",
    "        word2tdf[word]=tf_idf\n",
    "        \n",
    "    X=[]\n",
    "    for sentence in wordlist:\n",
    "        vec=[0]*word_total\n",
    "        for word in sentence:\n",
    "            if word in word2index and word in word2tdf:\n",
    "                vec[word2index[word]]=word2tdf[word]\n",
    "        X.append(vec)\n",
    "        \n",
    "    X_test=[]\n",
    "    for sencente in x:\n",
    "        vec=[0]*word_total\n",
    "        for word in sentence:\n",
    "            if word in word2index and word in word2tdf:\n",
    "                vec[word2index[word]]=word2tdf[word]\n",
    "        X_test.append(vec)\n",
    "            \n",
    "    \n",
    "    return X,X_test\n",
    "            \n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 对于用户的输入问题，找到相似度最高的TOP5问题，并把5个潜在的答案做返回"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def top5results(input_q):\n",
    "    \"\"\"\n",
    "    给定用户输入的问题 input_q, 返回最有可能的TOP 5问题。这里面需要做到以下几点：\n",
    "    1. 对于用户的输入 input_q 首先做一系列的预处理，然后再转换成tf-idf向量（利用上面的vectorizer)\n",
    "    2. 计算跟每个库里的问题之间的相似度\n",
    "    3. 找出相似度最高的top5问题的答案\n",
    "    \"\"\"\n",
    "   \n",
    "    qlist,alist=read_corpus()\n",
    "  \n",
    "\n",
    "    input_x=[]\n",
    "    input_x.append(input_q)\n",
    "    \n",
    "    X,X_test=Vectorization(qlist,input_x)\n",
    "    \n",
    "    X=np.array(X)\n",
    "\n",
    "    X_test=np.array(X_test)\n",
    "\n",
    "    index2sim={}\n",
    "    #使用点积求解相似度\n",
    "    \n",
    "    for i in range(X.shape[0]):\n",
    "        similarity=np.dot(X_test[0],X[i])\n",
    "        index2sim[i]=similarity\n",
    "        \n",
    "    sorted(index2sim.values(),reverse=True)\n",
    "\n",
    "    top_index=[]\n",
    "    for i in index2sim:\n",
    "        top_index.append(i)\n",
    "    \n",
    "    answer=[]\n",
    "    for i in top_index:\n",
    "        answer.append(alist[i])\n",
    "    return answer[:5]  # 返回相似度最高的问题对应的答案，作为TOP5答案    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 利用倒排表的优化。 \n",
    "上面的算法，一个最大的缺点是每一个用户问题都需要跟库里的所有的问题都计算相似度。假设我们库里的问题非常多，这将是效率非常低的方法。 这里面一个方案是通过倒排表的方式，先从库里面找到跟当前的输入类似的问题描述。然后针对于这些candidates问题再做余弦相似度的计算。这样会节省大量的时间。\n",
    "- 倒排表的思路是层层过滤\n",
    "- 也就是不是用所有的问题作为比对集合，而是筛选出候选。对于输入的问题的每一个词，看看他出现在那些句子中，将这些句子放在一起，也就是候选集合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getInvertedTable(input_q,qlist):\n",
    "    word2doc={}\n",
    "    input_q=input_q.split()\n",
    "    for word in input_q:\n",
    "        doc=[]\n",
    "        for i in range(len(qlist)):\n",
    "            if word in qlist[i]:\n",
    "                doc.append(i)\n",
    "        word2doc[word]=doc\n",
    "    return word2doc\n",
    "                  \n",
    "def top5results_invidx(input_q):\n",
    "    \"\"\"\n",
    "    给定用户输入的问题 input_q, 返回最有可能的TOP 5问题。这里面需要做到以下几点：\n",
    "    1. 利用倒排表来筛选 candidate\n",
    "    2. 对于用户的输入 input_q 首先做一系列的预处理，然后再转换成tf-idf向量（利用上面的vectorizer)\n",
    "    3. 计算跟每个库里的问题之间的相似度\n",
    "    4. 找出相似度最高的top5问题的答案\n",
    "    \"\"\"\n",
    "    qlist,alist=read_corpus()\n",
    "    #1.建立到排表\n",
    "    word2doc=getInvertedTable(input_q,qlist)\n",
    "    input_x=[]\n",
    "    input_x.append(input_q)\n",
    "    X,X_test=Vectorization(qlist,input_x)\n",
    "    \n",
    "    #合并候选doc\n",
    "    doc_candidate=[]\n",
    "    for word in word2doc:\n",
    "        doc_candidate+=word2doc[word]\n",
    "    doc_set=list(set(doc_candidate))\n",
    "    \n",
    "    doc2freq={}\n",
    "    \n",
    "    #对doc按照频率计数\n",
    "    num=0\n",
    "    for doc in doc_set:\n",
    "        for d in doc_candidate:\n",
    "            if doc==d:\n",
    "                num+=1\n",
    "        doc2freq[doc]=num\n",
    "    \n",
    "    sorted(doc2freq.values(),reverse=True)\n",
    "    \n",
    "    #候选doc的数量 选取前80%\n",
    "    doc_num=len(doc2freq)\n",
    "    \n",
    "    doc_select=int(doc_num*0.8)\n",
    "    \n",
    "    X_candidate=[]\n",
    "    i=0\n",
    "    for doc in doc2freq:\n",
    "        X_candidate.append(X[doc])\n",
    "        i+=1\n",
    "        if i>doc_select:\n",
    "            break\n",
    "            \n",
    "    X_candidate=np.array(X_candidate)\n",
    "    X_test=np.array(X_test)\n",
    "    index2sim={}\n",
    "    \n",
    "    #和候选列表进行计算相似度\n",
    "    for i in range(X_candidate.shape[0]):\n",
    "        sim=np.dot(X_test[0],X_candidate[i])\n",
    "        index2sim[doc_set[i]]=sim\n",
    "       \n",
    "    sorted(index2sim.values(),reverse=True)\n",
    "    answer=[]\n",
    "    \n",
    "    for index in index2sim:\n",
    "        answer.append(alist[index])\n",
    "        \n",
    "    return answer[:10]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 基于词向量的文本表示\n",
    "上面所用到的方法论是基于词袋模型（bag-of-words model）。这样的方法论有两个主要的问题：1. 无法计算词语之间的相似度  2. 稀疏度很高。 在2.7里面我们\n",
    "讲采用词向量作为文本的表示。词向量方面需要下载： https://nlp.stanford.edu/projects/glove/ （请下载glove.6B.zip），并使用d=100的词向量（100维）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#emb = # 读取每一个单词的嵌入。这个是 D*H的矩阵，这里的D是词典库的大小， H是词向量的大小。 这里面我们给定的每个单词的词向量，那句子向量怎么表达？\n",
    "      # 其中，最简单的方式 句子向量 = 词向量的平均（出现在问句里的）， 如果给定的词没有出现在词典库里，则忽略掉这个词。\n",
    "    \n",
    "def read_glove():\n",
    "    glove_path='glove.6B.100d.txt'\n",
    "    with open(glove_path,encoding='utf-8') as f:\n",
    "        wordset=set()\n",
    "        word2vec={}\n",
    "        \n",
    "        for line in f:\n",
    "            line=line.strip().split()\n",
    "            wordset.add(line[0])\n",
    "            word2vec[line[0]]=np.array([line[1:]],dtype=float)\n",
    "    return word2vec\n",
    "\n",
    "def glove_vectorization(word2vec,input_str):\n",
    "    sentence=input_str.split()\n",
    "    X=np.zeros((1,100))\n",
    "    for word in sentence:\n",
    "        if word in word2vec:\n",
    "            X+=word2vec[word]\n",
    "    X=X*1.0/(len(sentence))\n",
    "    return X\n",
    "    \n",
    "def top5results_emb(input_q):\n",
    "    \"\"\"\n",
    "    给定用户输入的问题 input_q, 返回最有可能的TOP 5问题。这里面需要做到以下几点：\n",
    "    1. 利用倒排表来筛选 candidate\n",
    "    2. 对于用户的输入 input_q，转换成句子向量\n",
    "    3. 计算跟每个库里的问题之间的相似度\n",
    "    4. 找出相似度最高的top5问题的答案\n",
    "    \"\"\"\n",
    "    qlist,alist=read_corpus()\n",
    "    #1.建立到排表\n",
    "    word2doc=getInvertedTable(input_q,qlist)\n",
    "    \n",
    "    doc_candidate=[]\n",
    "    for word in word2doc:\n",
    "        doc_candidate+=word2doc[word]\n",
    "    doc_set=list(set(doc_candidate))\n",
    "    \n",
    "    #使用平均向量来表示 这样就不用考虑句子长度了\n",
    "    word2vec=read_glove()\n",
    "    \n",
    "    index2sim={}\n",
    "    X_test=glove_vectorization(word2vec,input_q).transpose(1,0)\n",
    "    for doc in doc_set:\n",
    "        input_str=qlist[doc]\n",
    "        X_q=glove_vectorization(word2vec,input_str)\n",
    "        sim=np.dot(X_q,X_test)\n",
    "        index2sim[doc]=sim\n",
    "    sorted(index2sim.values(),reverse=True)\n",
    "    \n",
    "    answer=[]\n",
    "    for index in index2sim:\n",
    "        answer.append(alist[index])\n",
    "        \n",
    "    return answer[:5]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
